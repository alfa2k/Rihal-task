{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Machine Learning Challenge\n","\n","**Done by:** Fateen Ahmed\n","**Email:** fateenahmed.2k@gmail.com\n","\n","## Objectives\n","\n","**Level 1: The Basics** - To Develop a model that categorizes news articles into their respective categories.\n","\n","**Level 2: The Intermediate** - To Generate abstracts that summarize the articles clearly and concisely.\n","\n","**Level 3: The Advanced** - To Produce captions for each news article's image that accurately reflect the content.\n","\n","**Level 4: The Mastery** - To Implement a real-time UI web app for inference, facilitating user interaction.\n","\n","**Level 5: The Hero** - To Special focus on detecting articles related to Palestine and categorizing them under a new subcategory called \"FreePalestine\".\n","\n","\n","---\n"]},{"cell_type":"markdown","metadata":{},"source":["### Level 1: The Basics"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"IW9_y1_MDX04"},"outputs":[{"name":"stdout","output_type":"stream","text":["        section                                           headline  \\\n","0       Theater  Before 'Moonlight' and 'The Walking Dead,' a F...   \n","1    Television  What's on TV Wednesday: 'Crip Camp' and 'Dark ...   \n","2        Sports  Rays Stick to Their Plan and Reach a 2nd World...   \n","3  Art & Design    For Robert Rauschenberg, No Artist Is an Island   \n","4       Theater  Jimmy Buffett's 'Margaritaville' Musical Sets ...   \n","\n","                                         article_url  \\\n","0  https://www.nytimes.com/2017/02/21/theater/dan...   \n","1  https://www.nytimes.com/2020/03/25/arts/televi...   \n","2  https://www.nytimes.com/2020/10/17/sports/base...   \n","3  https://www.nytimes.com/2017/05/11/arts/design...   \n","4  https://www.nytimes.com/2017/06/05/theater/jim...   \n","\n","                                             article  \\\n","0  Danai Gurira and Andre Holland in a theater at...   \n","1  CRIP CAMP: A DISABILITY REVOLUTION (2020) Stre...   \n","2  The Tampa Bay Rays told Charlie Morton it woul...   \n","3  We tend to think of artists as natural loners,...   \n","4  Jimmy Buffett's tropical paradise will land in...   \n","\n","                                            abstract  \\\n","0  André Holland first saw Danai Gurira at New Yo...   \n","1  The documentary \"Crip Camp: A Disability Revol...   \n","2  Guided by the club's strict principles, Manage...   \n","3  A writer travels from Brooklyn to Lafayette, L...   \n","4  \"Escape to Margaritaville,\" which features new...   \n","\n","                             article_id  \\\n","0  952a8b15-63f6-5e41-8f23-82dfbb33d1fa   \n","1  67b1b62f-43ea-59d0-bf93-a94c11845820   \n","2  3da821ea-b93b-5a09-99d7-45e1711b73f0   \n","3  becf4474-d1fa-5722-b829-251530aa942b   \n","4  9511a127-e616-5bf5-80c6-553da4010c19   \n","\n","                                               image  \\\n","0  https://static01.nyt.com/images/2017/02/22/art...   \n","1  https://static01.nyt.com/images/2020/03/25/art...   \n","2  https://static01.nyt.com/images/2020/10/19/spo...   \n","3  https://static01.nyt.com/images/2017/05/14/art...   \n","4  https://static01.nyt.com/images/2017/06/06/art...   \n","\n","                                             caption  \\\n","0  Danai Gurira and André Holland in a theater at...   \n","1  Judy Heumann in &ldquo;Crip Camp: A Disability...   \n","2  The Rays celebrated after the final out of the...   \n","3  Robert Rauschenberg performing in \"Pelican\" in...   \n","4  Jimmy Buffett, performing in 2016. His musical...   \n","\n","                               image_id  \n","0  952a8b15-63f6-5e41-8f23-82dfbb33d1fa  \n","1  67b1b62f-43ea-59d0-bf93-a94c11845820  \n","2  3da821ea-b93b-5a09-99d7-45e1711b73f0  \n","3  becf4474-d1fa-5722-b829-251530aa942b  \n","4  9511a127-e616-5bf5-80c6-553da4010c19  \n"]}],"source":["# Initially the dataset is loaded from local machine.\n","import pandas as pd\n","\n","training_data_path = '/Users/fateenahmed/Downloads/N24News 2/news/nytimes_train.json'\n","testing_data_path = '/Users/fateenahmed/Downloads/N24News 2/news/nytimes_test.json'\n","\n","# Loading into dataframes\n","train_dataframe = pd.read_json(training_data_path)\n","test_dataframe = pd.read_json(testing_data_path)\n","\n","# Displaying the first few rows\n","print(train_dataframe.head())"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Column names in the training dataset: ['section', 'headline', 'article_url', 'article', 'abstract', 'article_id', 'image', 'caption', 'image_id']\n","\n","Distribution of news categories in the training dataset:\n"," Opinion            2437\n","Art & Design       2431\n","Television         2419\n","Music              2416\n","Travel             2413\n","Real Estate        2413\n","Books              2412\n","Health             2409\n","Theater            2409\n","Sports             2407\n","Science            2387\n","Fashion & Style    2385\n","Food               2385\n","Movies             2384\n","Technology         2376\n","Dance              2365\n","Media              2363\n","Style              2147\n","Automobiles        1456\n","Economy            1398\n","Your Money         1020\n","Global Business     955\n","Education           672\n","Well                529\n","Name: section, dtype: int64\n","\n","Missing values in each column of the training dataset:\n"," section        0\n","headline       0\n","article_url    0\n","article        0\n","abstract       0\n","article_id     0\n","image          0\n","caption        0\n","image_id       0\n","dtype: int64\n"]}],"source":["# Initial Analysis\n","\n","# Checking the columns, distribution, and missing values.\n","print(\"Column names in the training dataset:\", train_dataframe.columns.tolist())\n","print(\"\\nDistribution of news categories in the training dataset:\\n\", train_dataframe['section'].value_counts())\n","print(\"\\nMissing values in each column of the training dataset:\\n\", train_dataframe.isnull().sum())"]},{"cell_type":"markdown","metadata":{},"source":["The next process is text preprocessing which is essential for preparing raw text for NLP tasks. The Natural Language Toolkit (`nltk`) library is utilised as it provides a set of text processing tools and is easy to use.\n","\n","Text Preprocessing Tasks :\n","\n","Converting all text to lowercase to ensure uniformity\n","\n","Removing URLs using `re` library, as they do not contribute to the understanding of the text's content.\n","\n","Eliminating punctuation and numbers, focusing the analysis on words which carry the semantic weight of the text.\n","\n","Tokenizing using he `word_tokenize` function from `nltk` splits the cleaned text into individual words or tokens. This breaks down text into manageable units for analysis.\n","\n","Removing Common words such as \"and\", \"is\", or \"in\" are removed using a predefined list of stopwords from `nltk.corpus.stopwords`. These words are generally considered noise in text analysis because they occur frequently across texts of all topics and provide little unique information about the content of any single document.\n","\n","The tokens that remain after stopword removal are joined back into a single string, providing a cleaned version of the text for further processing or analysis."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["0    moonlight walking dead friendship born classro...\n","1    whats tv wednesday crip camp dark phoenix crip...\n","2    rays stick plan reach nd world series tampa ba...\n","3    robert rauschenberg artist island tend think a...\n","4    jimmy buffetts margaritaville musical sets bro...\n","Name: combined_text, dtype: object\n"]}],"source":["import re\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","import nltk\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","english_stop_words = set(stopwords.words('english'))\n","\n","def preprocess_text_article(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    tokens = word_tokenize(text)\n","    tokens = [word for word in tokens if word not in english_stop_words]\n","    return ' '.join(tokens)\n","\n","train_dataframe['combined_text'] = train_dataframe['headline'] + \" \" + train_dataframe['article']\n","train_dataframe['combined_text'] = train_dataframe['combined_text'].apply(preprocess_text_article)\n","\n","print(train_dataframe['combined_text'].head())"]},{"cell_type":"markdown","metadata":{},"source":["Then for modeling, the TF-IDF (Term Frequency-Inverse Document Frequency) is utlised via `TfidfVectorizer` to convert our combined text data into a matrix of numerical features, focusing on the top 10,000 most frequent words to balance computational efficiency with feature representation. This transformation enables the ML algorithms to process and learn from textual data. Following feature extraction, we split our dataset into training and validation sets using `train_test_split`, allocating 20% of the data for validation. This split is essential for training our models on one subset of the data and evaluating."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n","X_features = tfidf_vectorizer.fit_transform(train_dataframe['combined_text'])\n","y_labels = train_dataframe['section']\n","\n","X_train_features, X_validation_features, y_train_labels, y_validation_labels = train_test_split(\n","    X_features, y_labels, test_size=0.2, random_state=50)"]},{"cell_type":"markdown","metadata":{},"source":["To classify news articles into their respective categories, the Logistic Regression model from `sklearn.linear_model` is used. Logistic Regression is well-suited for multiclass classification tasks and offers a good balance between simplicity and performance for text classification.\n","\n","First the Logistic Regression model is initialised with an increased `max_iter` parameter to ensure convergence, given the potentially large and complex dataset. Then the model is then trained (`fit`) on training dataset, consisting of TF-IDF features (`X_train_features`) and their corresponding labels (`y_train_labels`).\n","\n","After training, the model is used to predict the categories of articles in the validation set (`X_validation_features`). The performance of the model is evaluated using the `classification_report` from `sklearn.metrics`, which provides key metrics such as precision, recall, and F1-score for each category. This evaluation helps us understand how well the model can generalize to unseen data."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","   Art & Design       0.86      0.91      0.88       474\n","    Automobiles       0.94      0.93      0.94       259\n","          Books       0.89      0.91      0.90       518\n","          Dance       0.97      0.94      0.96       478\n","        Economy       0.90      0.84      0.87       273\n","      Education       0.79      0.81      0.80       128\n","Fashion & Style       0.76      0.71      0.73       485\n","           Food       0.87      0.92      0.89       478\n","Global Business       0.85      0.80      0.83       184\n","         Health       0.83      0.92      0.87       513\n","          Media       0.83      0.85      0.84       474\n","         Movies       0.83      0.90      0.86       480\n","          Music       0.93      0.92      0.92       471\n","        Opinion       0.87      0.87      0.87       520\n","    Real Estate       0.92      0.93      0.92       452\n","        Science       0.88      0.88      0.88       461\n","         Sports       0.94      0.98      0.96       456\n","          Style       0.52      0.47      0.49       419\n","     Technology       0.87      0.91      0.89       499\n","     Television       0.88      0.89      0.89       460\n","        Theater       0.91      0.90      0.90       521\n","         Travel       0.85      0.84      0.85       486\n","           Well       0.79      0.38      0.51       109\n","     Your Money       0.92      0.78      0.84       200\n","\n","       accuracy                           0.86      9798\n","      macro avg       0.86      0.84      0.85      9798\n","   weighted avg       0.86      0.86      0.86      9798\n","\n"]}],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","\n","logistic_regression_model = LogisticRegression(max_iter=1000)\n","logistic_regression_model.fit(X_train_features, y_train_labels)\n","\n","y_predicted_labels = logistic_regression_model.predict(X_validation_features)\n","\n","print(classification_report(y_validation_labels, y_predicted_labels))"]},{"cell_type":"markdown","metadata":{},"source":["The model achieved an overall accuracy of 86% in classifying news articles into their respective categories, demonstrating strong performance across most of the 24 categories. Categories like 'Dance', 'Sports', and 'Real Estate' were among the top performers with F1-scores above 0.92, indicating the model's effectiveness in these areas. However, categories such as 'Style' and 'Well' exhibited lower F1-scores, suggesting room for improvement."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","   Art & Design       0.81      0.91      0.85       489\n","    Automobiles       0.98      0.77      0.86       266\n","          Books       0.73      0.80      0.76       476\n","          Dance       0.98      0.94      0.96       499\n","        Economy       0.71      0.64      0.67       286\n","      Education       0.00      0.00      0.00       132\n","Fashion & Style       0.69      0.75      0.72       465\n","           Food       0.91      0.80      0.85       450\n","Global Business       0.95      0.21      0.35       192\n","         Health       0.62      0.91      0.74       458\n","          Media       0.78      0.83      0.80       456\n","         Movies       0.93      0.43      0.59       469\n","          Music       0.90      0.90      0.90       516\n","        Opinion       0.41      0.85      0.55       478\n","    Real Estate       0.75      0.95      0.84       493\n","        Science       0.95      0.75      0.84       498\n","         Sports       0.90      0.95      0.92       500\n","          Style       0.57      0.18      0.27       432\n","     Technology       0.74      0.89      0.81       476\n","     Television       0.68      0.87      0.76       475\n","        Theater       0.87      0.89      0.88       494\n","         Travel       0.66      0.83      0.74       453\n","           Well       0.00      0.00      0.00       121\n","     Your Money       1.00      0.08      0.15       224\n","\n","       accuracy                           0.75      9798\n","      macro avg       0.73      0.67      0.66      9798\n","   weighted avg       0.77      0.75      0.73      9798\n","\n"]}],"source":["#Option 2 - Naive Bayes Classifier\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.pipeline import make_pipeline\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","X_train, X_test, y_train, y_test = train_test_split(train_dataframe['combined_text'], train_dataframe['section'], test_size=0.2, random_state=42)\n","\n","model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n","\n","model.fit(X_train, y_train)\n","\n","y_pred = model.predict(X_test)\n","\n","print(classification_report(y_test, y_pred, zero_division=0))"]},{"cell_type":"markdown","metadata":{},"source":["### Level 2: The Intermediate"]},{"cell_type":"markdown","metadata":{},"source":["Next the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization and cosine similarity is used to identify and compile the most informative sentences from a text article into a concise summary. Initially, the article is split into individual sentences, which are then transformed into a matrix of TF-IDF features. By calculating the cosine similarity among these sentence vectors, the relative importance of each sentence is assesed based on its similarity to others, presuming that sentences with higher overall similarity scores better capture the main themes of the article. These sentences are ranked, and the top ones are selected to construct the summary, with the number of sentences chosen based on the num_sentences parameter."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Article:\n"," We tend to think of artists as natural loners, off in their studios, wrestling with their inner selves. But \"Robert Rauschenberg: Among Friends,\" which opens at the Museum of Modern Art on May 21, points us in a different direction. It situates Rauschenberg's work amid that of two dozen fellow artists who provided an audience for one another in New York City in the '50s and '60s, the years of bohemia's final flourish. Not that exchanges between artists are ever simple. Rauschenberg's \"Bed,\" for instance, is a landmark painting from 1955 that stands about six feet tall, with a stapled-on pillow, a cotton sheet splotched with red and yellow, and rivulets of white pigment dripping onto a patchwork quilt. Like any true masterpiece, \"Bed\" can support multiple readings. You can see it, for starters, as a brilliant sendup of the overt emotionalism of Abstract Expressionism and say that it puts '50s-style paint-slinging to bed.\n","\n","In his defense, Rauschenberg, who died in 2008, at 82, could have cited Picasso's oft-quoted remark about how \"good artists copy, great artists steal.\" Granted, Picasso was referring to the theft of ideas, as opposed to actual objects. But for Rauschenberg, objects were the idea. It was a wholly optimistic belief. He delighted in the scruffy plenitude of the world, and he wanted to stretch the boundaries of painting to make room for things besides canvas and oil paint. His combines, as he called them, came to include crumpled newspapers and neckties and even a taxidermied goat. He did not invent assemblage, but he endowed it with scale, supersized it. Rauschenberg was charismatic and engaging. He saw himself as a work in progress (self-assembly required), and even his name was subject to revision. His parents named him Milton Ernest Rauschenberg. When he declared himself an artist, he changed his name to Bob. After people assumed, reasonably, that his name was actually Robert, he changed it to that.\n","\n","The MoMA exhibition, \"Among Friends,\" has been conceived by the curator Leah Dickerman as an \"open monograph,\" to borrow her revelatory phrase. It presents Rauschenberg as an essential connector not only among artists but also between the art world as a whole and the neighboring worlds of avant-garde dance and performance. His collaborations were frequent. They ranged from one-time improvisations -- such as his black-streaked \"Automobile Tire Print,\" which he made by having the musician John Cage drive his car over 20 sheets of paper in the street -- to a decade of sustained creativity as the resident designer of the Merce Cunningham Dance Company. In art-history textbooks, Rauschenberg is invariably paired with Jasper Johns as a historic figure who \"ushered in the Pop art era.\" But today that description seems dated and inadequate. Their work has none of Pop's screaming irony, none of its poster-like graphic speed. Johns, who turns 87 this month, forever changed the way we think about process, building up the surfaces of his paintings with slow, sensuous, wax-laden marks that seem to fold the flow of his time into his imagery. His achievement was to imbue American art with a seriousness, a philosophical largeness, that had previously belonged to European culture. Rauschenberg, by contrast, was interested in flouting rules and establishing freedoms, in making the art world a more open place.\n","\n","Rauschenberg's ethos of inclusiveness was probably rooted as much in his belief in social equality as in the hardships of his childhood. Born in 1925, he grew up during the Depression years in Port Arthur, Tex., an oil-refining town in the swampy southeastern part of the state. Not enough has been said about his dyslexia, which remained undiagnosed in his childhood. He later recalled that he harbored deep shame over his inability to keep up at school. He couldn't read, he said, because the letter \"r\" kept jumping off the page. He generally printed in capital letters, and his spelling was inventive. In a 1954 costume drawing for the Cunningham dance \"Minutiae,\" Rauschenberg refers to \"tourqoise\" and \"torqoise,\" to \"fushia\" and \"fuchsia.\" \"He told me so many times that he thought he was dumb when he was young,\" recalled his sister, Janet Begneaud, when we met in March in Lafayette, La., where the family eventually settled. Instead of poring over books, Rauschenberg had his own style of learning. He could imagine a sentence as a series of pictures, and cutout photographs from magazines proliferated on the walls of his boyhood bedroom. He became an attentive listener who picked up information from everyone he met. During World War II, he served in the Navy as a psychiatric nurse.\n","\n","Even so, having grown up amid the intolerance of the South, he learned to avoid certain topics at home. One of them was his homosexuality. \"I doubt Mother knew that it existed in the world,\" Ms. Begneaud said. \"I guarantee she had no idea.\" His parents were Christian fundamentalists who went to church two times every Sunday. Rauschenberg had ample opportunity to hear about the torments of hell, which piqued his imagination. In 1958, in an application for a Guggenheim grant, he proposed that he illustrate Dante's \"Inferno\" in 34 drawings, one for each canto. He was passed over for the grant, but he did complete the series of drawings, as the MoMA show will attest. They're among his best works, combining freely sketched passages with images from newspapers and magazines; they translate a 14th-century epic into a language of his own devising. The \"Inferno\" drawings represent, among other things, a triumph over Rauschenberg's dyslexia, signaling his love of literature. \"I had a book of Botticelli's wonderful illustrations of the 'Inferno,'\" Mr. Johns noted recently, referring to the years when he lived a flight below Rauschenberg in a loft building on Front Street. \"I believe that Bob's looking at that triggered his own.\" Does Dante count as one of Rauschenberg's collaborators? Probably more than Charles Atlas, a video artist prized for his work with choreographers. Now 68, Mr. Atlas is still lanky and boyish, with sideburns that are dyed psychedelic orange. \"Someone had the good idea to hire me,\" he said with a chuckle when we met recently at MoMA. \"It's the first time they have hired an artist to do anything like this.\" Mr. Atlas is designing the video portion of the show, including a nine-channel installation to commemorate \"9 Evenings,\" a series of quixotic performances that Rauschenberg spearheaded in 1966 with his engineer-friend Billy Kluver, in the hope of forming a brave new alliance between art and technology. \"Bob was so out there,\" recalled Susan Weil, a native Manhattanite who met him in Paris as an art student at the Academie Julian in the summer after she finished high school. He was 22 and studying on the G.I. Bill. In the fall, he followed her to Black Mountain College. They married in 1950 and moved to West 95th Street, into a cramped studio apartment. The MoMA show will open with their collaborations -- namely, the so-called \"blueprints,\" life-size photograms in which their bodies appear as white silhouettes bathed against a Prussian blue background.\n","\n","Summarized Article:\n"," He delighted in the scruffy plenitude of the world, and he wanted to stretch the boundaries of painting to make room for things besides canvas and oil paint. He was passed over for the grant, but he did complete the series of drawings, as the MoMA show will attest. Rauschenberg's ethos of inclusiveness was probably rooted as much in his belief in social equality as in the hardships of his childhood. They ranged from one-time improvisations -- such as his black-streaked \"Automobile Tire Print,\" which he made by having the musician John Cage drive his car over 20 sheets of paper in the street -- to a decade of sustained creativity as the resident designer of the Merce Cunningham Dance Company. It situates Rauschenberg's work amid that of two dozen fellow artists who provided an audience for one another in New York City in the '50s and '60s, the years of bohemia's final flourish.\n"]}],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","import nltk\n","\n","def summarize_article_tf_idf(article_text, num_sentences=5):\n","\n","    sentences = nltk.sent_tokenize(article_text)\n","    \n","    tfidf_vectorizer = TfidfVectorizer()\n","    sentence_tfidf_vectors = tfidf_vectorizer.fit_transform(sentences)\n","    \n","    sentence_cosine_similarity = cosine_similarity(sentence_tfidf_vectors)\n","    \n","    sentence_scores = sentence_cosine_similarity.sum(axis=1)\n","    \n","    ranked_sentence_indices = np.argsort(sentence_scores, axis=0)[::-1]\n","    top_sentences = [sentences[index] for index in ranked_sentence_indices.flatten()][:num_sentences]\n","    \n","    article_summary = ' '.join(top_sentences)\n","    return article_summary\n","\n","# Example case\n","example_text = train_dataframe.iloc[3]['article']  # Ensure the column name matches\n","print(\"Original Article:\\n\", example_text)\n","print(\"\\nSummarized Article:\\n\", summarize_article_tf_idf(example_text))"]},{"cell_type":"markdown","metadata":{},"source":["Next the spaCy model is utilised, which is used to detect named entities in the text. The function then iterates through each sentence and checks if it contains any of the named entities identified by spaCy. If so, the sentence's score is incremented, indicating that sentences containing named entities are considered more important for the summary. This approach is based on the intuition that sentences with named entities might carry more informational weight, making them good candidates for inclusion in the summary."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["import spacy\n","\n","nlp_model = spacy.load(\"en_core_web_sm\")\n","\n","def boost_scores_with_named_entities(sentences, named_entity_list, sentence_scores):\n","    for index, sentence in enumerate(sentences):\n","       \n","        if any(entity.text in sentence for entity in named_entity_list):\n","            sentence_scores[index] += 1\n","    \n","    return sentence_scores"]},{"cell_type":"markdown","metadata":{},"source":["The sentence are transformed into a semantic vector representation by averaging the word vectors of all tokens in the sentence. Then the spaCy model is loaded that includes word vectors, processes the input sentence to generate a document object, and then averages the vectors of tokens that have vector representations. If a sentence has no tokens with vectors or is empty, it returns a zero vector of a predefined length."]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import spacy\n","import numpy as np\n","\n","nlp_model = spacy.load(\"en_core_web_sm\")\n","\n","def compute_sentence_vector(sentence_text):\n","    doc = nlp_model(sentence_text)\n","    \n","    sentence_vector = np.mean([token.vector for token in doc if token.has_vector], axis=0)\n","    \n","    if len(doc) == 0 or not np.any(sentence_vector):\n","        return np.zeros((nlp_model.vocab.vectors_length,))\n","    \n","    return sentence_vector"]},{"cell_type":"markdown","metadata":{},"source":["The summaries are updated using the spaCy model that allows for named entity recognition and semantic analysis."]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Article:\n"," CRIP CAMP: A DISABILITY REVOLUTION (2020) Stream on Netflix. This documentary, the latest offering from Barack Obama and Michelle Obama's production company, draws a direct line between a Catskills summer camp and the American disability rights movement of the 1970s. Directed by Jim LeBrecht and Nicole Newnham, the film begins by focusing on Camp Jened, which was founded in the early 1950s and served as a community for campers with disabilities. But it eventually shifts focus to look at the adult lives of some of the camp's alumni, several of whom became prominent activists. In his review for The New York Times, Ben Kenigsberg wrote that the film \"unfolds from a perspective of lived experience.\" Newnham and LeBrecht, he added, \"deftly juggle a large cast of characters past and present, accomplishing the not-so-easy task of making all the personalities distinct.\"\n","\n","DARK PHOENIX (2019) 9 p.m. on HBO. In an interview with The Times last year, the actress Sophie Turner discussed the moment when Simon Kinberg, the writer and director of \"Dark Phoenix,\" made clear how much the movie would rely on Turner's performance. \"I just [expletive] my pants right there and then,\" Turner said. But she made it through. This most recent \"X-Men\" movie casts Turner, a star best known for \"Game of Thrones,\" as Jean Grey, a young mutant whose abnormally potent powers threaten to get the better of her. Michael Fassbender and James McAvoy reprise their roles as Magneto and Professor X, alongside Jessica Chastain as an alien modeled partly on Tilda Swinton. For the most part, Manohla Dargis wrote in her review for The Times, Kinberg \"just moves characters from point A to B, pausing for face-to-face heart to hearts before the next blowout.\" But, she added, \"the mayhem is generally coherent and executed with clean, crisp special effects, even if Kinberg settles for slo-mo cliches.\"\n","\n","BLACKHAT (2015) 5:20 p.m. on FXM. Cybercrime has incendiary results in \"Blackhat,\" a thriller directed by Michael Mann. The story begins with an explosion at a Hong Kong nuclear facility. Enter Chris Hemsworth and Tang Wei, who play unlikely heroes tasked with tracking down the mysterious hacker responsible for the attack. The result is \"by turns brutal and sentimental, lovely and lurid, as serious as the grave and blissfully preposterous,\" Manohla Dargis wrote in her review for The Times. She called the film \"a spectacular work of unhinged moviemaking.\"\n","\n","EARTH'S SACRED WONDERS 10 p.m. on PBS (check local listings). A team of gardeners takes on a dangerous plant-trimming job on the towers of Angkor Wat, the sprawling Cambodian temple complex, in the first episode of this new series. The show profiles people who have been drawn to religious landmarks around the world. Also featured in the first episode: A paramedic in Jerusalem who helps those who are fasting in during Ramadan, and a monk-in-training at the Shaolin Temple in China.\n","\n","Summarized Article:\n"," In an interview with The Times last year, the actress Sophie Turner discussed the moment when Simon Kinberg, the writer and director of \"Dark Phoenix,\" made clear how much the movie would rely on Turner's performance. In his review for The New York Times, Ben Kenigsberg wrote that the film \"unfolds from a perspective of lived experience.\" For the most part, Manohla Dargis wrote in her review for The Times, Kinberg \"just moves characters from point A to B, pausing for face-to-face heart to hearts before the next blowout.\" Also featured in the first episode: A paramedic in Jerusalem who helps those who are fasting in during Ramadan, and a monk-in-training at the Shaolin Temple in China. Cybercrime has incendiary results in \"Blackhat,\" a thriller directed by Michael Mann.\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk.tokenize import sent_tokenize\n","import spacy\n","import numpy as np\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def enhanced_summarize_article(article_text, num_sentences=5):\n","    sentences = sent_tokenize(article_text)\n","    doc = nlp(article_text)\n","    named_entities = doc.ents\n","\n","    enhanced_vectors = np.array([compute_sentence_vector(sentence) for sentence in sentences])\n","\n","    cosine_matrix = cosine_similarity(enhanced_vectors)\n","\n","    scores = cosine_matrix.sum(axis=1)\n","\n","    scores = boost_scores_with_named_entities(sentences, named_entities, scores)\n","\n","    ranked_sentences = [sentences[i] for i in np.argsort(scores, axis=0)[::-1]]\n","    summary = ' '.join(ranked_sentences[:num_sentences])\n","    return summary\n","\n","# Example case\n","example_article = train_dataframe.iloc[1]['article']\n","print(\"Original Article:\\n\", example_article)\n","print(\"\\nSummarized Article:\\n\", enhanced_summarize_article(example_article, num_sentences=5))"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Summarized Article: In an interview with The Times last year, the actress Sophie Turner discussed the moment when Simon Kinberg, the writer and director of \"Dark Phoenix,\" made clear how much the movie would rely on Turner's performance. For the most part, Manohla Dargis wrote in her review for The Times, Kinberg \"just moves characters from point A to B, pausing for face-to-face heart to hearts before the next blowout.\" This documentary, the latest offering from Barack Obama and Michelle Obama's production company, draws a direct line between a Catskills summer camp and the American disability rights movement of the 1970s.\n"]}],"source":["import spacy\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","\n","# Assuming nlp has been loaded with spaCy and necessary functions defined:\n","nlp_model = spacy.load(\"en_core_web_sm\")\n","\n","def advanced_summarize_article(article_text, num_sentences=3):\n","    doc = nlp_model(article_text)\n","    sentences = [str(sent) for sent in list(doc.sents)]\n","    sentence_ranks = {}\n","\n","    # Extract text of named entities for checking their presence in sentences\n","    named_entities_texts = {ent.text.lower() for ent in doc.ents}\n","\n","    for i, sent in enumerate(sentences):\n","        sentence_ranks[i] = 0\n","        for token in nlp_model(sent):\n","            # Check if token's lowercased text is a named entity or if token is a noun/verb\n","            if token.text.lower() in named_entities_texts or token.pos_ in ('NOUN', 'VERB'):\n","                sentence_ranks[i] += 1\n","\n","    # Rank sentences based on their scores\n","    ranked_sentences = sorted(sentence_ranks.keys(), key=lambda k: sentence_ranks[k], reverse=True)\n","    summary_sentences = [sentences[i] for i in ranked_sentences[:num_sentences]]\n","\n","    # Join the selected sentences to form the summary\n","    summary = ' '.join(summary_sentences)\n","    return summary\n","\n","# Example case, ensure you replace with an actual article text or DataFrame access\n","example_article = train_dataframe.iloc[1]['article']\n","print(\"Summarized Article:\", advanced_summarize_article(example_article, num_sentences=3))"]},{"cell_type":"markdown","metadata":{},"source":["### Level 3: The Advanced"]},{"cell_type":"markdown","metadata":{},"source":["The next task is to generate captions for images associated with articles. This is done by combining the article's headline, abstract, and body into a single text, then using the spaCy NLP library to identify named entities, nouns, and verbs within this text, extracting these elements as key information for caption creation. Depending on whether an existing caption is present, the code either generates a new caption by incorporating the first identified noun and verb or improves the existing one by appending up to three named entities."]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/anaconda3/lib/python3.7/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n","  warnings.warn(Warnings.W108)\n"]},{"name":"stdout","output_type":"stream","text":["                                               image  \\\n","0  https://static01.nyt.com/images/2017/02/22/art...   \n","1  https://static01.nyt.com/images/2020/03/25/art...   \n","2  https://static01.nyt.com/images/2020/10/19/spo...   \n","3  https://static01.nyt.com/images/2017/05/14/art...   \n","4  https://static01.nyt.com/images/2017/06/06/art...   \n","\n","                                             caption  \\\n","0  Danai Gurira and André Holland in a theater at...   \n","1  Judy Heumann in &ldquo;Crip Camp: A Disability...   \n","2  The Rays celebrated after the final out of the...   \n","3  Robert Rauschenberg performing in \"Pelican\" in...   \n","4  Jimmy Buffett, performing in 2016. His musical...   \n","\n","                                   generated_caption  \n","0  Danai Gurira and André Holland in a theater at...  \n","1  Judy Heumann in &ldquo;Crip Camp: A Disability...  \n","2  The Rays celebrated after the final out of the...  \n","3  Robert Rauschenberg performing in \"Pelican\" in...  \n","4  Jimmy Buffett, performing in 2016. His musical...  \n"]}],"source":["import spacy\n","import pandas as pd\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","def combine_textual_data(headline, abstract, article_body):\n","    return f\"{headline} {abstract} {article_body}\"\n","\n","def extract_key_information_for_captions(text):\n","    doc = nlp(text)\n","    entities = [ent.text for ent in doc.ents if not ent.text.isspace()]  # Named entities\n","    nouns = [token.text for token in doc if token.pos_ == 'NOUN' and not token.text.isspace()]  # Nouns\n","    verbs = [token.text for token in doc if token.pos_ == 'VERB' and not token.text.isspace()]  # Verbs\n","    return entities, nouns, verbs\n","\n","def generate_or_improve_caption(existing_caption, entities, nouns, verbs):\n","    if existing_caption:\n","        return f\"{existing_caption} Featuring {', '.join(entities[:3])}.\" if entities else existing_caption\n","    else:\n","        return f\"Captured moment of {nouns[0]} in action with {verbs[0]}.\" if nouns and verbs else \"Image related to the article.\"\n","\n","def process_dataset_for_captions(df):\n","    texts = [combine_textual_data(row['headline'], row['abstract'], row['article']) for _, row in df.iterrows()]\n","\n","    entities_info = []\n","    for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\"], batch_size=100):\n","        entities = [ent.text for ent in doc.ents if not ent.text.isspace()]\n","        nouns = [token.text for token in doc if token.pos_ == 'NOUN' and not token.text.isspace()]\n","        verbs = [token.text for token in doc if token.pos_ == 'VERB' and not token.text.isspace()]\n","        entities_info.append((entities, nouns, verbs))\n","    \n","    for index, (entities, nouns, verbs) in enumerate(entities_info):\n","        new_caption = generate_or_improve_caption(df.at[index, 'caption'], entities, nouns, verbs)\n","        df.at[index, 'generated_caption'] = new_caption\n","\n","    return df\n","\n","enhanced_df = process_dataset_for_captions(train_dataframe)\n","print(enhanced_df[['image', 'caption', 'generated_caption']].head()) "]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["                                                 caption  \\\n","37421  View of Emerald Bay in Lake Tahoe from Inspira...   \n","40743  A metallic bronze ch&egrave;vre leather Herm&e...   \n","48227  The Swedish Academy's formal gathering at the ...   \n","47135  A Spanish protester criticizes Swiss requests ...   \n","46191  Jessica Grose, at work in her home, is the edi...   \n","\n","                                       generated_caption  \n","37421  bay, Lake Tahoe, Inspiration Point. View of Em...  \n","40743  metallic, egrave, bronze. A metallic bronze ch...  \n","48227  formal, Academy, Stockholm. The Swedish Academ...  \n","47135  protester, Spain, criticizes. A Spanish protes...  \n","46191  weekly, grose, NYT Parenting. Jessica Grose, a...  \n"]}],"source":["import pandas as pd\n","import spacy\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# Load spaCy model and NLTK data\n","nlp = spacy.load(\"en_core_web_sm\")\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Set of English stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","def extract_keywords(text):\n","    # Use spaCy for named entities\n","    doc = nlp(text)\n","    entities = [ent.text for ent in doc.ents if ent.text.lower() not in stop_words]\n","    \n","    # Use NLTK for frequent terms, excluding stopwords and non-alphabetic words\n","    words = word_tokenize(text.lower())\n","    common_terms = [word for word in words if word.isalpha() and word not in stop_words]\n","    \n","    # Frequency distribution of common terms, excluding entities\n","    fdist = FreqDist(common_terms)\n","    top_common_terms = [word for word, freq in fdist.most_common(3) if word not in entities]\n","    \n","    # Combine and deduplicate\n","    keywords = list(set(entities + top_common_terms))\n","    return keywords\n","\n","def generate_enhanced_caption(original_caption, max_length=100):\n","    keywords = extract_keywords(original_caption)\n","    if not keywords:\n","        # If no keywords are extracted, return the original or a shortened version\n","        return original_caption if len(original_caption) <= max_length else original_caption[:max_length-3] + \"...\"\n","    \n","    # Integrate keywords more contextually\n","    enhanced_portion = ', '.join(keywords[:3])\n","    new_caption = f\"{enhanced_portion}. {original_caption}\"\n","    \n","    # Shorten the caption if it exceeds the maximum length, trying to preserve the enhanced portion\n","    if len(new_caption) > max_length:\n","        cut_off_point = max_length - len(enhanced_portion) - 15\n","        original_caption_short = original_caption[:cut_off_point] + \"...\" if len(original_caption) > cut_off_point else original_caption\n","        new_caption = f\"{enhanced_portion}. {original_caption_short}\"\n","    \n","    return new_caption\n","\n","# Assuming 'train_df' is already loaded with your dataset\n","# Applying the enhanced caption generation to a random subset of 100 rows\n","subset_df = train_dataframe.sample(n=100, random_state=42)\n","subset_df['generated_caption'] = subset_df['caption'].apply(generate_enhanced_caption)\n","\n","# Displaying the original and generated captions for review\n","print(subset_df[['caption', 'generated_caption']].head())\n"]},{"cell_type":"markdown","metadata":{},"source":["In order to try and enhance image captions the named entities and common terms are extracted from the text. These keywords are incorporated into the original captions. Again,spaCy is used to identify specific, significant words or phrases as named entities, excluding stopwords. Then again, NLTK is applied for tokenizing the text and identifying frequent, relevant terms, excluding stopwords and non-alphabetic words."]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /Users/fateenahmed/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["                                             caption  \\\n","0  Danai Gurira and André Holland in a theater at...   \n","1  Judy Heumann in &ldquo;Crip Camp: A Disability...   \n","2  The Rays celebrated after the final out of the...   \n","3  Robert Rauschenberg performing in \"Pelican\" in...   \n","4  Jimmy Buffett, performing in 2016. His musical...   \n","\n","                                   generated_caption  \n","0  third-years', one, The Walking Dead, A few yea...  \n","1  Crip Camp', FXM, China, 2019, the Shaolin Temp...  \n","2  Milwaukee, Roy Halladay, Boston, Saturday, Bra...  \n","3  1954, Brooklyn, John Cage, Bob, Charles Atlas....  \n","4  the Marquis Theater, buffett, Jimmy Buffett's,...  \n"]}],"source":["import pandas as pd\n","import spacy\n","from nltk.tokenize import word_tokenize\n","from nltk.probability import FreqDist\n","import nltk\n","from nltk.corpus import stopwords\n","\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nlp = spacy.load(\"en_core_web_sm\")\n","stop_words = set(stopwords.words('english'))\n","\n","def extract_keywords(text):\n","    doc = nlp(text)\n","    entities = [ent.text for ent in doc.ents if ent.text.lower() not in stop_words]\n","    words = word_tokenize(text.lower())\n","    common_terms = [word for word in words if word.isalpha() and word not in stop_words]\n","    fdist = FreqDist(common_terms)\n","    top_common_terms = [word for word, freq in fdist.most_common(3) if word not in entities]\n","    keywords = list(set(entities + top_common_terms))\n","    return keywords\n","\n","def generate_enhanced_caption(row):\n","    \n","    consolidated_text = f\"{row['headline']}. {row['abstract']}. {row['article']}\"\n","    article_keywords = extract_keywords(consolidated_text)\n","\n","    caption_keywords = extract_keywords(row['caption']) if pd.notna(row['caption']) else []\n","    \n","    combined_keywords = list(dict.fromkeys(article_keywords + caption_keywords))[:5]\n","    \n","    enhanced_portion = ', '.join(combined_keywords)\n","    new_caption = f\"{enhanced_portion}. {row['caption']}\" if row['caption'] else f\"This image relates to {enhanced_portion}.\"\n","\n","    return new_caption[:100]\n","\n","subset_df = train_dataframe.head(100).copy() \n","\n","subset_df['generated_caption'] = subset_df.apply(generate_enhanced_caption, axis=1)\n","\n","print(subset_df[['caption', 'generated_caption']].head())"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Error loading punkt: <urlopen error [SSL:\n","[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n","[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n","[nltk_data] Error loading stopwords: <urlopen error [SSL:\n","[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n","[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"]},{"name":"stdout","output_type":"stream","text":["                 precision    recall  f1-score   support\n","\n","   Art & Design       0.89      0.92      0.90       489\n","    Automobiles       0.93      0.95      0.94       266\n","          Books       0.88      0.89      0.89       476\n","          Dance       0.97      0.95      0.96       499\n","        Economy       0.88      0.82      0.85       286\n","      Education       0.82      0.85      0.83       132\n","Fashion & Style       0.72      0.73      0.73       465\n","           Food       0.87      0.94      0.90       450\n","Global Business       0.87      0.84      0.85       192\n","         Health       0.80      0.88      0.84       458\n","          Media       0.82      0.89      0.85       456\n","         Movies       0.81      0.91      0.86       469\n","          Music       0.92      0.91      0.91       516\n","        Opinion       0.86      0.86      0.86       478\n","    Real Estate       0.92      0.93      0.93       493\n","        Science       0.88      0.88      0.88       498\n","         Sports       0.95      0.96      0.95       500\n","          Style       0.54      0.47      0.50       432\n","     Technology       0.88      0.89      0.89       476\n","     Television       0.90      0.84      0.87       475\n","        Theater       0.92      0.91      0.91       494\n","         Travel       0.89      0.85      0.87       453\n","           Well       0.81      0.41      0.55       121\n","     Your Money       0.88      0.79      0.83       224\n","\n","       accuracy                           0.86      9798\n","      macro avg       0.86      0.84      0.85      9798\n","   weighted avg       0.86      0.86      0.86      9798\n","\n","Running on local URL:  http://127.0.0.1:7863\n","\n","To create a public link, set `share=True` in `launch()`.\n"]},{"data":{"text/html":["<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["import gradio as gr\n","import pandas as pd\n","import spacy\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","import numpy as np\n","import nltk\n","from nltk.corpus import stopwords\n","import re\n","\n","nltk.download('punkt', quiet=True)\n","nltk.download('stopwords', quiet=True)\n","\n","# Initialize spaCy for English language\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Initialize the TF-IDF Vectorizer and Logistic Regression model\n","tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n","logistic_regression_model = LogisticRegression(max_iter=1000)\n","\n","def preprocess_text_article(text):\n","    text = text.lower()\n","    text = re.sub(r'http\\S+', '', text)\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    tokens = word_tokenize(text)\n","    english_stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in english_stop_words]\n","    return ' '.join(tokens)\n","\n","def compute_sentence_vector(sentence):\n","    doc = nlp(sentence)\n","    sentence_vector = np.mean([token.vector for token in doc if token.has_vector], axis=0)\n","    if np.isnan(sentence_vector).any():\n","        return np.zeros((nlp.vocab.vectors_length,))\n","    return sentence_vector\n","\n","def boost_scores_with_named_entities(sentences, named_entities, scores):\n","    named_entity_texts = {ent.text for ent in named_entities}\n","    for i, sentence in enumerate(sentences):\n","        if any(ent in sentence for ent in named_entity_texts):\n","            scores[i] += 1\n","    return scores\n","\n","def enhanced_summarize_article(article_text, num_sentences=5):\n","    sentences = sent_tokenize(article_text)\n","    doc = nlp(article_text)\n","    named_entities = doc.ents\n","    enhanced_vectors = np.array([compute_sentence_vector(sentence) for sentence in sentences])\n","    cosine_matrix = cosine_similarity(enhanced_vectors)\n","    scores = cosine_matrix.sum(axis=1)\n","    scores = boost_scores_with_named_entities(sentences, named_entities, scores)\n","    ranked_sentences = [sentences[i] for i in np.argsort(scores, axis=0)[::-1]]\n","    summary = ' '.join(ranked_sentences[:num_sentences])\n","    return summary\n","\n","def extract_keywords(text):\n","    doc = nlp(text)\n","    keywords = set(token.text.lower() for token in doc if token.is_alpha and not token.is_stop)\n","    return list(keywords)\n","\n","def load_data():\n","    training_data_path = '/Users/fateenahmed/Downloads/N24News 2/news/nytimes_train.json'  # Update this path\n","    train_dataframe = pd.read_json(training_data_path)\n","    train_dataframe['combined_text'] = train_dataframe['headline'] + \" \" + train_dataframe['article']\n","    train_dataframe['preprocessed_text'] = train_dataframe['combined_text'].apply(preprocess_text_article)\n","    return train_dataframe['preprocessed_text'], train_dataframe['section']\n","\n","def train_model():\n","    global tfidf_vectorizer, logistic_regression_model\n","    X, y = load_data()\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","    logistic_regression_model.fit(X_train_tfidf, y_train)\n","    \n","    # Optionally, evaluate the model on the test set\n","    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","    y_pred = logistic_regression_model.predict(X_test_tfidf)\n","    print(classification_report(y_test, y_pred))\n","\n","# Call train_model() to ensure models are trained and vectorizer is fitted\n","train_model()\n","\n","def predict_category(article_body):\n","    processed_text = preprocess_text_article(article_body)\n","    tfidf_features = tfidf_vectorizer.transform([processed_text])\n","    category = logistic_regression_model.predict(tfidf_features)[0]\n","    return str(category)\n","\n","def generate_abstract(article_body):\n","    return enhanced_summarize_article(article_body, num_sentences=5)\n","\n","def generate_enhanced_caption(article_title, article_body, image_caption):\n","    combined_text = f\"{article_title} {article_body}\"\n","    keywords = extract_keywords(combined_text + \" \" + image_caption)\n","    enhanced_caption = \", \".join(keywords[:5])\n","    return enhanced_caption if enhanced_caption else image_caption\n","\n","def process_article(article_title, article_body, image_caption_text=\"\"):\n","    category = predict_category(article_body)\n","    abstract = generate_abstract(article_body)\n","    enhanced_caption = generate_enhanced_caption(article_title, article_body, image_caption_text)\n","    return category, abstract, enhanced_caption\n","\n","interface = gr.Interface(\n","    fn=process_article,\n","    inputs=[\n","        gr.Textbox(label=\"Article Title\", placeholder=\"Enter Article Title Here...\"),\n","        gr.Textbox(label=\"Article Body\", placeholder=\"Enter Article Body Here...\", lines=7),\n","        gr.Textbox(label=\"Image Caption\", placeholder=\"Enter Image Caption Here...\")\n","    ],\n","    outputs=[\n","        gr.Text(label=\"Predicted Category\"),\n","        gr.Text(label=\"Generated Abstract\"),\n","        gr.Text(label=\"Enhanced Caption\")\n","    ],\n","    title=\"Ahmed's News Article Processing App\",\n","    description=\"This app predicts the category of news articles, generates abstracts, and enhances captions for images based on the provided image caption text.\"\n",")\n","\n","if __name__ == \"__main__\":\n","    interface.launch()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMhny/w5XD4e0jFnh/ApVqs","mount_file_id":"12r0_91B0KDHUrhnWoow6uRCi8JW8v3mk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":0}
